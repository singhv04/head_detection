{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb845d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd96b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26239bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7a2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88d33a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as fp:\n",
    "        names = fp.read().splitlines()\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d611c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_path = self.files[index % len(self.files)]\n",
    "        img = np.array(\n",
    "            Image.open(img_path).convert('RGB'),\n",
    "            dtype=np.uint8)\n",
    "\n",
    "        # Label Placeholder\n",
    "        boxes = np.zeros((1, 5))\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img, _ = self.transform((img, boxes))\n",
    "\n",
    "        return img_path, img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537ac6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy_np(x):\n",
    "    y = np.zeros_like(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a8eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgAug(object):\n",
    "    def __init__(self, augmentations=[]):\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Unpack data\n",
    "        img, boxes = data\n",
    "\n",
    "        # Convert xywh to xyxy\n",
    "        boxes = np.array(boxes)\n",
    "        boxes[:, 1:] = xywh2xyxy_np(boxes[:, 1:])\n",
    "\n",
    "        # Convert bounding boxes to imgaug\n",
    "        bounding_boxes = BoundingBoxesOnImage(\n",
    "            [BoundingBox(*box[1:], label=box[0]) for box in boxes],\n",
    "            shape=img.shape)\n",
    "\n",
    "        # Apply augmentations\n",
    "        img, bounding_boxes = self.augmentations(\n",
    "            image=img,\n",
    "            bounding_boxes=bounding_boxes)\n",
    "\n",
    "        # Clip out of image boxes\n",
    "        bounding_boxes = bounding_boxes.clip_out_of_image()\n",
    "\n",
    "        # Convert bounding boxes back to numpy\n",
    "        boxes = np.zeros((len(bounding_boxes), 5))\n",
    "        for box_idx, box in enumerate(bounding_boxes):\n",
    "            # Extract coordinates for unpadded + unscaled image\n",
    "            x1 = box.x1\n",
    "            y1 = box.y1\n",
    "            x2 = box.x2\n",
    "            y2 = box.y2\n",
    "\n",
    "            # Returns (x, y, w, h)\n",
    "            boxes[box_idx, 0] = box.label\n",
    "            boxes[box_idx, 1] = ((x1 + x2) / 2)\n",
    "            boxes[box_idx, 2] = ((y1 + y2) / 2)\n",
    "            boxes[box_idx, 3] = (x2 - x1)\n",
    "            boxes[box_idx, 4] = (y2 - y1)\n",
    "\n",
    "        return img, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d2eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeLabels(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        h, w, _ = img.shape\n",
    "        boxes[:, [1, 3]] /= w\n",
    "        boxes[:, [2, 4]] /= h\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "class AbsoluteLabels(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        h, w, _ = img.shape\n",
    "        boxes[:, [1, 3]] *= w\n",
    "        boxes[:, [2, 4]] *= h\n",
    "        return img, boxes\n",
    "\n",
    "\n",
    "class PadSquare(ImgAug):\n",
    "    def __init__(self, ):\n",
    "        self.augmentations = iaa.Sequential([\n",
    "            iaa.PadToAspectRatio(\n",
    "                1.0,\n",
    "                position=\"center-center\").to_deterministic()\n",
    "        ])\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        # Extract image as PyTorch tensor\n",
    "        img = transforms.ToTensor()(img)\n",
    "\n",
    "        bb_targets = torch.zeros((len(boxes), 6))\n",
    "        bb_targets[:, 1:] = transforms.ToTensor()(boxes)\n",
    "\n",
    "        return img, bb_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ac6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRANSFORMS = transforms.Compose([\n",
    "    AbsoluteLabels(),\n",
    "    PadSquare(),\n",
    "    RelativeLabels(),\n",
    "    ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6998fa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        img, boxes = data\n",
    "        img = F.interpolate(img.unsqueeze(0), size=self.size, mode=\"nearest\").squeeze(0)\n",
    "        return img, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d90dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_data_loader(img_path, batch_size, img_size, n_cpu):\n",
    "    \"\"\"Creates a DataLoader for inferencing.\n",
    "\n",
    "    :param img_path: Path to file containing all paths to validation images.\n",
    "    :type img_path: str\n",
    "    :param batch_size: Size of each image batch\n",
    "    :type batch_size: int\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation\n",
    "    :type n_cpu: int\n",
    "    :return: Returns DataLoader\n",
    "    :rtype: DataLoader\n",
    "    \"\"\"\n",
    "    dataset = ImageFolder(\n",
    "        img_path,\n",
    "        transform=transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)]))\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=n_cpu,\n",
    "        pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe302b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a61336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c46ad7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu(tensor):\n",
    "    return tensor.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2a0bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None):\n",
    "    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n",
    "    Returns:\n",
    "         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "\n",
    "    # Settings\n",
    "    # (pixels) minimum and maximum box width and height\n",
    "    max_wh = 4096\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 1.0  # seconds to quit after\n",
    "    multi_label = nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "\n",
    "    t = time.time()\n",
    "    output = [torch.zeros((0, 6), device=\"cpu\")] * prediction.shape[0]\n",
    "\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[x[..., 4] > conf_thres]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            # sort by confidence\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        # boxes (offset by class), scores\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "\n",
    "        output[xi] = to_cpu(x[i])\n",
    "\n",
    "        if (time.time() - t) > time_limit:\n",
    "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82347ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_d = []\n",
    "pre_data = []\n",
    "post_data = []\n",
    "model_infer = []\n",
    "model_nms = []\n",
    "def detect(model, dataloader, output_path, conf_thres, nms_thres):\n",
    "    \"\"\"Inferences images with model.\n",
    "\n",
    "    :param model: Model for inference\n",
    "    :type model: models.Darknet\n",
    "    :param dataloader: Dataloader provides the batches of images to inference\n",
    "    :type dataloader: DataLoader\n",
    "    :param output_path: Path to output directory\n",
    "    :type output_path: str\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    :return: List of detections. The coordinates are given for the padded image that is provided by the dataloader.\n",
    "        Use `utils.rescale_boxes` to transform them into the desired input image coordinate system before its transformed by the dataloader),\n",
    "        List of input image paths\n",
    "    :rtype: [Tensor], [str]\n",
    "    \"\"\"\n",
    "    # Create output directory, if missing\n",
    "    tss = time.time()\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "    img_detections = []  # Stores detections for each image index\n",
    "    imgs = []  # Stores image paths\n",
    "    tse = time.time()\n",
    "    setup_d.append(tse-tss)\n",
    "    for (img_paths, input_imgs) in tqdm.tqdm(dataloader, desc=\"Detecting\"):\n",
    "        # Configure input\n",
    "        tps = time.time()\n",
    "        input_imgs = Variable(input_imgs.type(Tensor))\n",
    "        tpe = time.time()\n",
    "        pre_data.append(tpe-tps)\n",
    "        # Get detections\n",
    "        with torch.no_grad():\n",
    "            tms = time.time()\n",
    "            detections = model(input_imgs)\n",
    "            tmi = time.time()\n",
    "            detections = non_max_suppression(detections, conf_thres, nms_thres)\n",
    "            tme = time.time()\n",
    "            model_infer.append(tmi-tms)\n",
    "            model_nms.append(tme-tmi)\n",
    "\n",
    "        # Store image and detections\n",
    "        tposts = time.time()\n",
    "        img_detections.extend(detections)\n",
    "        imgs.extend(img_paths)\n",
    "        tposte = time.time()\n",
    "        post_data.append(tposte-tposts)\n",
    "    return img_detections, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330f607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d17da0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_boxes(boxes, current_dim, original_shape):\n",
    "    \"\"\"\n",
    "    Rescales bounding boxes to the original shape\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
    "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
    "\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = current_dim - pad_y\n",
    "    unpad_w = current_dim - pad_x\n",
    "\n",
    "    # Rescale bounding boxes to dimension of original image\n",
    "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
    "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71bf2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_and_save_output_images(img_detections, imgs, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output images and stores them.\n",
    "\n",
    "    :param img_detections: List of detections\n",
    "    :type img_detections: [Tensor]\n",
    "    :param imgs: List of paths to image files\n",
    "    :type imgs: [str]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate through images and save plot of detections\n",
    "    for (image_path, detections) in zip(imgs, img_detections):\n",
    "        print(f\"Image {image_path}:\")\n",
    "        _draw_and_save_output_image(\n",
    "            image_path, detections, img_size, output_path, classes)\n",
    "\n",
    "\n",
    "def _draw_and_save_output_image(image_path, detections, img_size, output_path, classes):\n",
    "    \"\"\"Draws detections in output image and stores this.\n",
    "\n",
    "    :param image_path: Path to input image\n",
    "    :type image_path: str\n",
    "    :param detections: List of detections on image\n",
    "    :type detections: [Tensor]\n",
    "    :param img_size: Size of each image dimension for yolo\n",
    "    :type img_size: int\n",
    "    :param output_path: Path of output directory\n",
    "    :type output_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    \"\"\"\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(image_path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    # Rescale boxes to original image\n",
    "    detections = rescale_boxes(detections, img_size, img.shape[:2])\n",
    "    unique_labels = detections[:, -1].cpu().unique()\n",
    "    n_cls_preds = len(unique_labels)\n",
    "    # Bounding-box colors\n",
    "    cmap = plt.get_cmap(\"tab20b\")\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, n_cls_preds)]\n",
    "    bbox_colors = random.sample(colors, n_cls_preds)\n",
    "    for x1, y1, x2, y2, conf, cls_pred in detections:\n",
    "\n",
    "        # print(f\"\\t+ Label: {classes[int(cls_pred)]} | Confidence: {conf.item():0.4f}\")\n",
    "\n",
    "        box_w = x2 - x1\n",
    "        box_h = y2 - y1\n",
    "\n",
    "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "        # Create a Rectangle patch\n",
    "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor=\"none\")\n",
    "        # Add the bbox to the plot\n",
    "        ax.add_patch(bbox)\n",
    "        # Add label\n",
    "        # plt.text(\n",
    "        #     x1,\n",
    "        #     y1,\n",
    "        #     s=classes[int(cls_pred)],\n",
    "        #     color=\"white\",\n",
    "        #     verticalalignment=\"top\",\n",
    "        #     bbox={\"color\": color, \"pad\": 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis(\"off\")\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    filename = os.path.basename(image_path).split(\".\")[0]\n",
    "    output_path = os.path.join(output_path, f\"{filename}.png\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58db2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_directory(model_path, weights_path, img_path, classes, output_path,\n",
    "                     batch_size=8, img_size=416, n_cpu=8, conf_thres=0.5, nms_thres=0.5):\n",
    "    \"\"\"Detects objects on all images in specified directory and saves output images with drawn detections.\n",
    "\n",
    "    :param model_path: Path to model definition file (.cfg)\n",
    "    :type model_path: str\n",
    "    :param weights_path: Path to weights or checkpoint file (.weights or .pth)\n",
    "    :type weights_path: str\n",
    "    :param img_path: Path to directory with images to inference\n",
    "    :type img_path: str\n",
    "    :param classes: List of class names\n",
    "    :type classes: [str]\n",
    "    :param output_path: Path to output directory\n",
    "    :type output_path: str\n",
    "    :param batch_size: Size of each image batch, defaults to 8\n",
    "    :type batch_size: int, optional\n",
    "    :param img_size: Size of each image dimension for yolo, defaults to 416\n",
    "    :type img_size: int, optional\n",
    "    :param n_cpu: Number of cpu threads to use during batch generation, defaults to 8\n",
    "    :type n_cpu: int, optional\n",
    "    :param conf_thres: Object confidence threshold, defaults to 0.5\n",
    "    :type conf_thres: float, optional\n",
    "    :param nms_thres: IOU threshold for non-maximum suppression, defaults to 0.5\n",
    "    :type nms_thres: float, optional\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    dataloader = _create_data_loader(img_path, batch_size, img_size, n_cpu)\n",
    "    t2 = time.time()\n",
    "    model = load_model(model_path, weights_path)\n",
    "    t3 = time.time()\n",
    "    img_detections, imgs = detect(\n",
    "        model,\n",
    "        dataloader,\n",
    "        output_path,\n",
    "        conf_thres,\n",
    "        nms_thres)\n",
    "    t4 = time.time()\n",
    "    _draw_and_save_output_images(\n",
    "        img_detections, imgs, img_size, output_path, classes)\n",
    "    t5 = time.time()\n",
    "    gc.collect()\n",
    "    print(f\"---- Detections were saved to: '{output_path}' ----\")\n",
    "\n",
    "    print(\"Time taken by - Dataloader:{} Model_init:{} Inference:{} Post:{}\".format((t2-t1),(t3-t2),(t4-t3),(t5-t4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d6441a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adminspin/Desktop/nuclei_detection/yolo/batch_infer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9563c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'custom_yolo/yolov3.cfg'\n",
    "weights_path = 'custom_yolo/yolov3_last.weights'\n",
    "img_path = 'dummy'\n",
    "classes = load_classes('custom_yolo/obj.names')\n",
    "output_path = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7349352d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f2a05f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dummy/test1.jpg:\n",
      "Image dummy/test10.jpg:\n",
      "Image dummy/test11.jpg:\n",
      "Image dummy/test12.jpg:\n",
      "Image dummy/test13.jpg:\n",
      "Image dummy/test14.jpg:\n",
      "Image dummy/test15.jpg:\n",
      "Image dummy/test16.jpg:\n",
      "Image dummy/test17.jpg:\n",
      "Image dummy/test18.jpg:\n",
      "Image dummy/test19.jpg:\n",
      "Image dummy/test2.jpg:\n",
      "Image dummy/test20.jpg:\n",
      "Image dummy/test21.jpg:\n",
      "Image dummy/test22.jpg:\n",
      "Image dummy/test23.jpg:\n",
      "Image dummy/test24.jpg:\n",
      "Image dummy/test25.jpg:\n",
      "Image dummy/test26.jpg:\n",
      "Image dummy/test27.jpg:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adminspin/Desktop/nuclei_detection/yolo/yolo_env/lib/python3.6/site-packages/ipykernel_launcher.py:40: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dummy/test28.jpg:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adminspin/Desktop/nuclei_detection/yolo/yolo_env/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dummy/test29.jpg:\n",
      "Image dummy/test3.jpg:\n",
      "Image dummy/test30.jpg:\n",
      "Image dummy/test31.jpg:\n",
      "Image dummy/test32.jpg:\n",
      "Image dummy/test4.jpg:\n",
      "Image dummy/test5.jpg:\n",
      "Image dummy/test6.jpg:\n",
      "Image dummy/test7.jpg:\n",
      "Image dummy/test8.jpg:\n",
      "Image dummy/test9.jpg:\n",
      "---- Detections were saved to: 'output' ----\n",
      "Time taken by - Dataloader:0.0015575885772705078 Model_init:3.185830593109131 Inference:1.6619954109191895 Post:10.469681739807129\n",
      "Inference time: [0.35279345512390137]\n",
      "NMS time: [0.03302478790283203]\n",
      "Total Infer+Nms for each batch:[0.3858182430267334]; Total:0.3858182430267334\n",
      "Settup time :[0.0013203620910644531]; Preprocess data:[0.0060923099517822266] Postdetection:[1.239776611328125e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "detect_directory(model_path, weights_path, img_path, classes, output_path, batch_size=32, img_size=416, n_cpu=16, conf_thres=0.25, nms_thres=0.5)\n",
    "print(\"Inference time:\",model_infer)\n",
    "print(\"NMS time:\", model_nms)\n",
    "infer_nms_batches = [x + y for x, y in zip(model_infer, model_nms)]\n",
    "print(\"Total Infer+Nms for each batch:{}; Total:{}\".format(infer_nms_batches, sum(infer_nms_batches)))\n",
    "\n",
    "print(\"Settup time :{}; Preprocess data:{} Postdetection:{}\".format(setup_d,pre_data,post_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
